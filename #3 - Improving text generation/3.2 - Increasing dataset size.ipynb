{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Increasing dataset size\n",
    "\n",
    "The next thing we're going to try is to increase the size of our dataset. On the previous trainings we used a small subset of the book \"Don Quijote de La Mancha\" that contained 169KB of text.\n",
    "\n",
    "The problem is that we have to consider that what we're going to do is to teach Spanish to our RNN. And, let's be honest, it's quite difficult to learn a language from scratch by reading only 169K characters (a few chapters of a book); we'll learn some words and maybe even a few sentences, but it's very difficult to really learn the language. \n",
    "\n",
    "Therefore, in order to solve this, we'll greatly increase the size of the dataset. We'll use the entire \"Don Quijote de la Mancha\" book, and to it we'll append another very famous Spanish book, \"La Regenta\" by Leopoldo Alas. Combining both, we'll get a dataset of about 4MB (more than 20x the previous one). And, although this will slow down our training a lot, it will be with very high probability a very huge improvement in our code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 960M (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to read both books and to combine them into a single dataset, and then we'll proceed with the usual calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  3976780\n",
      "Total Vocab:  104\n",
      "Total Patterns:  3976680\n"
     ]
    }
   ],
   "source": [
    "# Load the books, merging them and covert the result to lowercase\n",
    "filename1 = \"El ingenioso hidalgo don Quijote de la Mancha.txt\"\n",
    "book1 = open(filename1).read()\n",
    "filename2 = \"La Regenta.txt\"\n",
    "book2 = open(filename2).read()\n",
    "book = book1 + book2\n",
    "\n",
    "# Create mapping of unique chars to integers, and its reverse\n",
    "chars = sorted(list(set(book)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Summarizing the loaded data\n",
    "n_chars = len(book)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab\n",
    "\n",
    "# Prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# Iterating over the book\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    sequence_in = book[i:i + seq_length]\n",
    "    sequence_out = book[i + seq_length]\n",
    "    \n",
    "    # Converting each char to its corresponding int\n",
    "    sequence_in_int = [char_to_int[char] for char in sequence_in]\n",
    "    sequence_out_int = char_to_int[sequence_out]\n",
    "\n",
    "    # Appending the result to the current data \n",
    "    dataX.append(sequence_in_int)\n",
    "    dataY.append(sequence_out_int)\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns\n",
    "\n",
    "# Reshaping X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# Normalizing\n",
    "X = X / float(n_vocab)\n",
    "# One hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Starting from a checkpoint (if we set one)\n",
    "checkpoint = \"\"\n",
    "if checkpoint:\n",
    "    model.load_weights(checkpoint)\n",
    "\n",
    "# Amount of epochs that we still have to run\n",
    "epochs_run = 0\n",
    "epochs_left = 50 - epochs_run\n",
    "\n",
    "# Define the checkpoints structure\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fitting the model\n",
    "model.fit(X, y, nb_epoch=epochs_left, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We won't see the results here because I've actually executed this code in another machine, not directly in the notebook; as you can imagine, this will take a loooooong time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We'll, so here we are again! If you're reading this once I've finished the notebook you won't notice the pause, but I'm writing this two weeks later than the previous paragraph. \n",
    "\n",
    "As I predicted, the NN took a looooooooong time to learn. Each one of the epochs required about 11 hours to finish! And besides, there's another important thing to take into account: the NN stopped generating weights after the 10th one, although the code is still running. I'd like to thing that it happened because the loss stopped decreasing at that point (what won't be as bad as it may seem, because due to the big size of the dataset we achieved quite good results even with few epochs), but we can't know it for sure at the moment; however, I will for sure update this notebook when I analyse it more precisely, so don't stop visiting it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that this part is explained, let's go back to what really matters: the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test our neural net, we'll use the two approaches tried before, in order to see the results achieved with each one: choosing the most probable character each iteration and using the output probabilities as the Probability Density Function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Preparing the prediction\n",
    "\n",
    "In this section we're going to include all the code that is common to both prediction methods (loading the weights, preparing the seed...) in order to avoid executing the same code twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the network weights\n",
    "filename = \"weights-improvement-09-1.5410.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" : A mi marido Sancho Panza, gobernador de la\n",
      "ínsula Barataria, que Dios prospere más años que a m \"\n"
     ]
    }
   ],
   "source": [
    "# Pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "starting_pattern = pattern  # saving a copy\n",
    "seed = ''.join([int_to_char[value] for value in pattern])\n",
    "print \"Seed:\"\n",
    "print \"\\\"\", seed, \"\\\"\"\n",
    "result_str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Most probable character\n",
    "The code to use here doesn't need much explanation, as is exactly the one we've used on previous notebooks. You can check them to see the reason for using this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate characters\n",
    "for i in range(500):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tresult_str += result\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Randomized prediction\n",
    "The code to use here doesn't need much explanation, as is exactly the one we've used on previous notebooks. You can check them to see the reason for using this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = starting_pattern  # Restoring the seed to its initial state\n",
    "result_str = \"\"\n",
    "\n",
    "# Generate characters\n",
    "for i in range(500):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "    \n",
    "    # Choosing the character randomly\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tprob_cum = np.cumsum(prediction[0])\n",
    "\trand_ind = np.random.rand()\n",
    "\tfor i in range(len(prob_cum)):\n",
    "\t    if (rand_ind <= prob_cum[i]) and (rand_ind > prob_cum[i-1]):\n",
    "\t        index = i\n",
    "\t        break\n",
    "            \n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tresult_str += result\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
