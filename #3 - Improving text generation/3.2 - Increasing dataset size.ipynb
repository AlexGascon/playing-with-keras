{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Increasing dataset size\n",
    "\n",
    "The next thing we're going to try is to increase the size of our dataset. On the previous trainings we used a small subset of the book \"Don Quijote de La Mancha\" that contained 169KB of text.\n",
    "\n",
    "The problem is that we have to consider that what we're going to do is to teach Spanish to our RNN. And, let's be honest, it's quite difficult to learn a language from scratch by reading only 169K characters (a few chapters of a book); we'll learn some words and maybe even a few sentences, but it's very difficult to really learn the language. \n",
    "\n",
    "Therefore, in order to solve this, we'll greatly increase the size of the dataset. We'll use the entire \"Don Quijote de la Mancha\" book, and to it we'll append another very famous Spanish book, \"La Regenta\" by Leopoldo Alas. Combining both, we'll get a dataset of about 4MB (more than 20x the previous one). And, although this will slow down our training a lot, it will be with very high probability a very huge improvement in our code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 960M (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to read both books and to combine them into a single dataset, and then we'll proceed with the usual calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the books, merging them and covert the result to lowercase\n",
    "filename1 = \"El ingenioso hidalgo don Quijote de la Mancha.txt\"\n",
    "book1 = open(filename1).read()\n",
    "filename2 = \"La Regenta.txt\"\n",
    "book2 = open(filename2).read()\n",
    "book = book1 + book2\n",
    "book = book.lower()\n",
    "\n",
    "# Create mapping of unique chars to integers, and its reverse\n",
    "chars = sorted(list(set(book)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Summarizing the loaded data\n",
    "n_chars = len(book)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab\n",
    "\n",
    "# Prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# Iterating over the book\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    sequence_in = book[i:i + seq_length]\n",
    "    sequence_out = book[i + seq_length]\n",
    "    \n",
    "    # Converting each char to its corresponding int\n",
    "    sequence_in_int = [char_to_int[char] for char in sequence_in]\n",
    "    sequence_out_int = char_to_int[sequence_out]\n",
    "\n",
    "    # Appending the result to the current data \n",
    "    dataX.append(sequence_in_int)\n",
    "    dataY.append(sequence_out_int)\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns\n",
    "\n",
    "# Reshaping X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# Normalizing\n",
    "X = X / float(n_vocab)\n",
    "# One hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Starting from a checkpoint (if we set one)\n",
    "checkpoint = \"\"\n",
    "if checkpoint:\n",
    "    model.load_weights(checkpoint)\n",
    "\n",
    "# Amount of epochs that we still have to run\n",
    "epochs_run = 0\n",
    "epochs_left = 50 - epochs_run\n",
    "\n",
    "# Define the checkpoints structure\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fitting the model\n",
    "model.fit(X, y, nb_epoch=epochs_left, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We won't see the results here because I've actually executed this code in another machine, not directly in the notebook; as you can imagine, this will take a loooooong time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
