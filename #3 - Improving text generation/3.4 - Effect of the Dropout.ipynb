{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - Effect of the Dropout\n",
    "The next thing we're going to try is to change the Dropout of the NN. As we explained in the notebook 1.1, dropout is a way of reducing overfitting in the network by randomly ignoring some of its units during training time. \n",
    "\n",
    "Its effectivness comes because we'll usually have more units in the intermediate layers (and therefore, more connections) than parameters with actual data in our inputs; therefore, part of the information we're processing is nothing but noise. By randomly dropping some of the units, we force the NN to reduce its necessity of specific units, distributing the weights more evenly among the remaining ones. \n",
    "\n",
    "We can see some graphic examples of the effects of dropout on the folllowing images:\n",
    "\n",
    "At network level\n",
    "![Dropout network](http://i.imgur.com/1PZWwVs.png)\n",
    "\n",
    "At unit level\n",
    "![Dropout unit](http://i.imgur.com/VboQmL5.png)\n",
    "\n",
    "More info can be found on the publication *Dropout: A Simple Way to Prevent Neural Networks from Overfitting* from Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov [\\[1\\]](http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. Changes in the architecture\n",
    "In previous notebooks we've been using a Dropout of 0.2 in our hidden units, and no dropouts on the input or output units. This time, we won't still use dropout on input or output units (as they have a huge probability of negatively affecting the accuracy of our system), but we'll change the dropout probability in the hidden layers.\n",
    "\n",
    "We'll start by increasing it to a probability of 0.5, and depending on the results obtained we'll choose which value to try next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(We only include the architecture definition part, as the rest of the code remains unchanged)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
