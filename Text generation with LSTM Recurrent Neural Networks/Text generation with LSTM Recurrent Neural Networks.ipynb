{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with LSTM Recurrent Neural Networks\n",
    "\n",
    "By Alex Gasc√≥n Bononad - alexgascon.93@gmail.com\n",
    "\n",
    "## 0. Introduction\n",
    "\n",
    "### 0.1. Introduction to the Notebook\n",
    "In this notebook we'll follow the following tutorial: http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/ [1]\n",
    "\n",
    "As the title states, we're going to use Python and Keras to create a LSTM Recurrent Neural Network to use for text generation. We'll start by following what the tutorial specifies and then we'll try some variations that let us experimentate and get different results\n",
    "\n",
    "\n",
    "### 0.2. Introduction to the concepts\n",
    "\n",
    "There are several concepts that will be useful to know during this tutorial\n",
    "\n",
    "#### Text generation\n",
    "Text Generation or Natural language generation (NLG) is the natural language processing task of generating natural language from a machine representation system. It could be said an NLG system is like a translator that converts data into a natural language representation. \n",
    "\n",
    "NLG may be viewed as the opposite of natural language understanding: whereas in natural language understanding the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into words.\n",
    "\n",
    "_More info:_\n",
    "https://en.wikipedia.org/wiki/Natural_language_generation [2]\n",
    "\n",
    "#### LSTM Recurrent Neural Networks\n",
    "LSTM is the acronym for Long short-term memory, a type of Recurrent Neural Networks capable of learning long-term dependencies. They are explicitly designed to avoid the long-term dependency problem, so they don't struggle at all to remember information for long periods of time.\n",
    "\n",
    "_More info:_ <br />\n",
    "http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf [3] <br />\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/ [4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Description of the problem\n",
    "As we've previously explained, the purpose of this notebook is to generate text using a Neural Network. \n",
    "\n",
    "In order to do this, we need a big amount of information to train the NN, and in this case, we've chosen the book _Alice's Adventures in Wonderland_, by Lewis Carroll. It's no longer protected with copyright, and we can find it on [Project Gutenberg](https://www.gutenberg.org/). Specifically, we're going to use the text version of the book.\n",
    "\n",
    "First of all, we'll remove the header and footer of the book, as it contains generic info about Project Gutenberg that we don't want to include in the training set. This doesn't mean that it isn't important (in fact, as we're using information they've hosted and distributed, the least we can do for the project is to read these information), but that we don't want to analyse it as it may greatly affect our output. You can find the edited .txt file in the same directory as this notebook, with the name \"Alice's adventures in Wonderland.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Develop a Small LSTM Recurrent Neural Network\n",
    "\n",
    "Now let's start the coding! The first steps will be to load the libraries that we'll need during this notebook and to read the text file. Besides, we'll convert it to lowercase in order to simplify the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the book and converting to lowercase\n",
    "filename = \"Alice's adventures in Wonderland.txt\"\n",
    "book = open(filename).read()\n",
    "book = book.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll prepare the data to be modelled by the Neural Network. In order to do this we'll map each character to their corresponding ASCII integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a sorted list where every char in the book appears just once\n",
    "chars = sorted(list(set(book)))\n",
    "# Getting a dictionary that representa the mapping of every char (key) to their corresponding int (value)\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we have the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in the book:  144586\n",
      "Unique characters in the book:  46\n"
     ]
    }
   ],
   "source": [
    "print(\"Total characters in the book: \", len(book))\n",
    "print(\"Unique characters in the book: \", len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we'll start to define the training data for our LSTM RNN. In order to follow the original tutorial, what we'll do is to split the whole book into strings of 100 characters, and consider the following one as the expected output. \n",
    "\n",
    "We will slide this 100-character window along the whole book one chatacter at a time, so each char will be learned 100 times (with the exception of the first 100 characters). \n",
    "\n",
    "As a simpler way of explaining this: if instead of a window of length 100 we'll use one of length 5, the following ones will be some of the training inputs:\n",
    "\n",
    "CHAPT --> E <br/>\n",
    "HAPTE --> R <br/>\n",
    "wonde --> r <br/>\n",
    "onder --> l <br/>\n",
    "nderl --> a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:  144486\n"
     ]
    }
   ],
   "source": [
    "# Preparing the dataset of input-output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# Iterating over the book\n",
    "for i in range(0, len(book) - seq_length):\n",
    "    sequence_in = book[i : i + seq_length]\n",
    "    sequence_out = book[i + seq_length]\n",
    "    \n",
    "    # Converting each char to its corresponding int\n",
    "    sequence_in_int = [char_to_int[char] for char in sequence_in]\n",
    "    sequence_out_int = char_to_int[sequence_out]\n",
    "    \n",
    "    # Appending the result to the current data \n",
    "    dataX.append(sequence_in_int)\n",
    "    dataY.append(sequence_out_int)\n",
    "    \n",
    "print(\"Total patterns: \", len(dataX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the total amount of input patterns is exactly the total amount of characters in the text - 100. This, of course, is not a coincidence: we have a pattern to predict each of the characters, except for the first 100 (the value of our seq_length) because they don't have enough preceding characters to be predicted. \n",
    "\n",
    "The next step will be to transform the training data to make it suitable for its use in Keras.\n",
    "\n",
    "We'll do this in three steps: the first one is to transform the list of input sequences into the form _[samples, time steps, features]_ expected by an LSTM RNN. \n",
    "- **Samples**: the amount of input elements to use during our training\n",
    "- **Time steps**: indicates how long will a single input be kept in our network (i.e. how many iterations will occur before it leaves the network)\n",
    "- **Features**: How many features does the network predict each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshaping X to be [samples, time_steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, len(sequence_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to normalize the outputs between 0 and 1, in order to make the patterns easier to learn by the LSTM network, that by default uses the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizing between 0 and 1\n",
    "X = X / float(len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to convert the output patterns into a one hot enconding. This encoding is represented with a group of bits only one single can have a value of 1, with the other ones being 0. \n",
    "\n",
    "Let's see some examples of binary to one-hot conversions:\n",
    "_000 --> 00000001\n",
    "001 --> 00000010\n",
    "010 --> 00000100\n",
    "011 --> 00001000\n",
    "100 --> 00010000\n",
    "101 --> 00100000\n",
    "110 --> 01000000\n",
    "111 --> 10000000_\n",
    "\n",
    "In our case, the one-hot encoding will be represented by an array formed by 46 items (the amount of unique characters in our text) in which only one of them will be a 1. For example, the value of \"n\" (integer value 31) will be the following one:\n",
    "\n",
    "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. 0.  0.  0.  0.  0.  0.  0.  0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
